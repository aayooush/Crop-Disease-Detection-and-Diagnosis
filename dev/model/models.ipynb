{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I do not claim full ownership of following given models, As it is part my work and part inspiriration from various Research Papers and Code Snippets from Github and StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VuwqK7s65Z1f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Activation, PReLU, BatchNormalization, MaxPool2D, MaxPooling2D, GlobalAveragePooling2D, Add, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '../input/plantvillage-dataset/color/'\n",
    "training = os.path.join(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qtgA8KViHc71"
   },
   "outputs": [],
   "source": [
    "N_CLASSES = len(os.listdir(training))\n",
    "IMG_W = 224\n",
    "IMG_H = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                                                training,\n",
    "                                                target_size=(IMG_W, IMG_H),\n",
    "                                                class_mode='categorical',\n",
    "                                                subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                                                training,\n",
    "                                                target_size=(IMG_W, IMG_H),\n",
    "                                                class_mode='categorical',\n",
    "                                                subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alexnet(in_shape=(224,224,3), n_classes= N_CLASSES):\n",
    "    in_layer = Input(in_shape)\n",
    "    \n",
    "    conv1 = Conv2D(96, 11, strides=4, activation='relu')(in_layer)\n",
    "    pool1 = MaxPool2D(3, 2)(conv1)\n",
    "\n",
    "    conv2 = Conv2D(256, 5, strides=1, padding='same', activation='relu')(pool1)\n",
    "    pool2 = MaxPool2D(3, 2)(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(384, 3, strides=1, padding='same', activation='relu')(pool2)\n",
    "    conv4 = Conv2D(256, 3, strides=1, padding='same', activation='relu')(conv3)\n",
    "    pool3 = MaxPool2D(3, 2)(conv4)\n",
    "    \n",
    "    flattened = Flatten()(pool3)\n",
    "    \n",
    "    dense1 = Dense(4096, activation='relu')(flattened)\n",
    "    drop1 = Dropout(0.5)(dense1)\n",
    "    dense2 = Dense(4096, activation='relu')(drop1)\n",
    "    drop2 = Dropout(0.5)(dense2)\n",
    "    \n",
    "    preds = Dense(n_classes, activation='softmax')(drop2)\n",
    "\n",
    "    model = Model(in_layer, preds)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch < 20:\n",
    "        return 0.001\n",
    "    elif epoch > 20 and epoch <= 40:\n",
    "        return 0.0001\n",
    "    else: \n",
    "        return 0.00001\n",
    "    \n",
    "tensorboard = TensorBoard(\n",
    "                        log_dir='logs/{}'.format(int(time())), \n",
    "                        histogram_freq=0, \n",
    "                        write_graph=True, \n",
    "                        write_grads=False, \n",
    "                        write_images=True, \n",
    "                        embeddings_freq=0, \n",
    "                        embeddings_layer_names=None, \n",
    "                        embeddings_metadata=None, \n",
    "                        embeddings_data=None, \n",
    "                        update_freq='epoch')\n",
    "\n",
    "modelcheckpoint = ModelCheckpoint(\n",
    "                                filepath = 'model-alexnet-{}.h5'.format(int(time())), \n",
    "                                monitor = 'val_acc',\n",
    "                                verbose = 1,\n",
    "                                save_best_only = 1,\n",
    "                                save_weights_only = False,\n",
    "                                mode = 'max',\n",
    "                                save_freq = 'epoch' )\n",
    "\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "callback = [tensorboard, modelcheckpoint, lr_scheduler]\n",
    "model = Alexnet()\n",
    "model.summary()\n",
    "history = model.fit_generator(train_generator, epochs=50, callbacks = callback, verbose = 1, shuffle=True, validation_data = validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwrDnT9p5Z2M"
   },
   "outputs": [],
   "source": [
    "def DCNN_Network(in_shape=(224,224,3), n_classes=N_CLASSES):\n",
    "    in_layer = Input(in_shape)\n",
    "    \n",
    "    conv1 = Conv2D(64, 5, strides=1, padding=\"same\", activation='relu')(in_layer)\n",
    "    conv2 = Conv2D(64, 5, strides=1, padding=\"same\", activation='relu')(conv1)\n",
    "    bat1 = BatchNormalization()(conv2)\n",
    "    pool1 = MaxPool2D(pool_size=(2,2),strides=(2,2))(bat1)\n",
    "    \n",
    "    conv3 = Conv2D(64, 3, strides=1, padding=\"same\", activation='relu')(pool1)\n",
    "    conv4 = Conv2D(64, 3, strides=1, padding=\"same\", activation='relu')(conv3)\n",
    "    bat2 = BatchNormalization()(conv4)\n",
    "    pool2 = MaxPool2D(pool_size=(2,2),strides=(2,2))(bat2)\n",
    "    \n",
    "    conv5 = Conv2D(128, 3, strides=1, padding=\"same\", activation='relu')(pool2)\n",
    "    conv6 = Conv2D(128, 3, strides=1, padding=\"same\", activation='relu')(conv5)\n",
    "    bat3 = BatchNormalization()(conv6)\n",
    "    pool3 = MaxPool2D(pool_size=(2,2),strides=(2,2))(bat3)\n",
    "    \n",
    "    conv7 = Conv2D(128, 3, strides=1, padding=\"same\", activation='relu')(pool3)\n",
    "    conv8 = Conv2D(128, 3, strides=1, padding=\"same\", activation='relu')(conv7)\n",
    "    bat4 = BatchNormalization()(conv8)\n",
    "    pool4 = MaxPool2D(pool_size=(2,2),strides=(2,2))(bat4)\n",
    "    \n",
    "    conv9 = Conv2D(256, 3, strides=1, padding=\"same\", activation='relu')(pool4)\n",
    "    conv10 = Conv2D(256, 3, strides=1, padding=\"same\", activation='relu')(conv9)\n",
    "    bat5 = BatchNormalization()(conv10)\n",
    "    pool5 = MaxPool2D(pool_size=(2,2),strides=(2,2))(bat5)\n",
    "    \n",
    "    \n",
    "    flattened = Flatten()(pool5)\n",
    "    dense1 = Dense(512, activation='relu')(flattened)\n",
    "    dense2 = Dense(512, activation='relu')(dense1)\n",
    "    preds = Dense(n_classes, activation='softmax')(dense2)\n",
    "\n",
    "    model = Model(in_layer, preds)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch < 20:\n",
    "        return 0.001\n",
    "    elif epoch > 20 and epoch <= 40:\n",
    "        return 0.0001\n",
    "    else: \n",
    "        return 0.00001\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "                        log_dir='logs/{}'.format(int(time())), \n",
    "                        histogram_freq=0, \n",
    "                        write_graph=True, \n",
    "                        write_grads=False, \n",
    "                        write_images=True, \n",
    "                        embeddings_freq=0, \n",
    "                        embeddings_layer_names=None, \n",
    "                        embeddings_metadata=None, \n",
    "                        embeddings_data=None, \n",
    "                        update_freq='epoch')\n",
    "\n",
    "modelcheckpoint = ModelCheckpoint(\n",
    "                                filepath = 'model-dcnn-{}.h5'.format(int(time())), \n",
    "                                monitor = 'val_acc',\n",
    "                                verbose = 1,\n",
    "                                save_best_only = 1,\n",
    "                                save_weights_only = False,\n",
    "                                mode = 'max',\n",
    "                                save_freq = 'epoch' )\n",
    "\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "callback = [tensorboard, modelcheckpoint, lr_scheduler]\n",
    "model = DCNN_Network()\n",
    "\n",
    "history = model.fit_generator(train_generator, epochs=50, callbacks = callback, verbose = 1, shuffle=True, validation_data = validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeModel(nbChannels, shape1, shape2, nbClasses, nbRCL=5, nbFilters=128, filtersize = 3):\n",
    "    model = BuildRCNN(nbChannels, shape1, shape2, nbClasses, nbRCL, nbFilters, filtersize)\n",
    "    return model\n",
    "\n",
    "def BuildRCNN(nbChannels, shape1, shape2, nbClasses, nbRCL, nbFilters, filtersize):\n",
    "    \n",
    "    def RCL_block(l_settings, l, pool=True, increase_dim=False):\n",
    "        input_num_filters = l_settings.output_shape[1]\n",
    "        if increase_dim:\n",
    "            out_num_filters = input_num_filters*2\n",
    "        else:\n",
    "            out_num_filters = input_num_filters\n",
    "\n",
    "        conv1 = Conv2D(out_num_filters, 1, 1, padding='same')\n",
    "        stack1 = conv1(l)   \t\n",
    "        stack2 = BatchNormalization()(stack1)\n",
    "        stack3 = PReLU()(stack2)\n",
    "        \n",
    "        conv2 = Conv2D(out_num_filters, filtersize, 1, padding='same', kernel_initializer = 'he_normal')\n",
    "        stack4 = conv2(stack3)\n",
    "\n",
    "        stack5 = Add()([stack1, stack4])\n",
    "        stack6 = BatchNormalization()(stack5)\n",
    "        stack7 = PReLU()(stack6)\n",
    "    \n",
    "        conv3 = Conv2D(out_num_filters, filtersize, 1, padding='same', weights = conv2.get_weights())\n",
    "        stack8 = conv3(stack7)\n",
    "        stack9 = Add()([stack1, stack8])\n",
    "        stack10 = BatchNormalization()(stack9)\n",
    "        stack11 = PReLU()(stack10)    \n",
    "        \n",
    "        conv4 = Conv2D(out_num_filters, filtersize, 1, padding='same', weights = conv2.get_weights())\n",
    "        stack12 = conv4(stack11)\n",
    "        stack13 = Add()([stack1, stack12])\n",
    "        stack14 = BatchNormalization()(stack13)\n",
    "        stack15 = PReLU()(stack14)    \n",
    "        \n",
    "        if pool:\n",
    "            stack16 = MaxPooling2D((2, 2), (2,2))(stack15) \n",
    "            stack17 = Dropout(0.1)(stack16)\n",
    "        else:\n",
    "            stack17 = Dropout(0.1)(stack15)\n",
    "            \n",
    "        return stack17\n",
    "\n",
    "    input_img = Input(shape=(shape1, shape2, nbChannels))\n",
    "    conv_l = Conv2D(nbFilters, 2,2, padding='same', activation='relu')\n",
    "    l = conv_l(input_img)\n",
    "    for n in range(nbRCL):\n",
    "        if n % 2 ==0:\n",
    "            l = RCL_block(conv_l, l, pool=False)\n",
    "        else:\n",
    "            l = RCL_block(conv_l, l, pool=True)\n",
    "    \n",
    "    out = Flatten()(l)        \n",
    "    l_out = Dense(nbClasses, activation = 'softmax')(out)\n",
    "    \n",
    "    model = Model(inputs = input_img, outputs = l_out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = makeModel(nbChannels = 3, shape1 = IMG_W, shape2 = IMG_H, nbClasses = N_CLASSES, nbRCL=6, nbFilters=96, filtersize = 3)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.001\n",
    "    elif epoch > 5 and epoch <= 10:\n",
    "        return 0.0001\n",
    "    else: \n",
    "        return 0.00001\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs1/RCNN-BatchNorm-{}\".format(time()), \n",
    "                          histogram_freq=0, \n",
    "                          write_graph=True,\n",
    "                          update_freq='epoch')\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "callbacks = [tensorboard, lr_scheduler]\n",
    "\n",
    "history = model.fit_generator(train_generator, epochs=20, callbacks = callbacks, verbose = 1, shuffle=True, validation_data = validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "    def __init__(self, n, channels_first=False, initial_lr=0.01, nb_epochs=50):\n",
    "        self.n = n\n",
    "        self.initial_lr = initial_lr\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.weight_decay = 0.0005\n",
    "        self.channels_first = channels_first\n",
    "        self.data_format = \"channels_first\" if channels_first else \"channels_last\"\n",
    "        self.bn_axis = 1 if channels_first else -1\n",
    "        self.model = self.make_model()\n",
    "        self.model.summary()\n",
    "\n",
    "    def subsumpling(self, output_channels, input_tensor):\n",
    "        return Conv2D(output_channels, kernel_size=1, strides=(2,2), data_format=self.data_format, kernel_regularizer=l2(self.weight_decay))(input_tensor)\n",
    "\n",
    "    def block(self, channles, input_tensor):\n",
    "        shortcut = input_tensor\n",
    "        x = BatchNormalization(axis=self.bn_axis)(input_tensor)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv2D(channles, kernel_size=3, padding=\"same\", data_format=self.data_format,kernel_regularizer=l2(self.weight_decay))(x)\n",
    "        x = BatchNormalization(axis=self.bn_axis)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv2D(channles, kernel_size=3, padding=\"same\", data_format=self.data_format,kernel_regularizer=l2(self.weight_decay))(x)\n",
    "        return Add()([x, shortcut])\n",
    "\n",
    "    def make_model(self):\n",
    "        input = Input(shape=(3, IMG_W, IMG_H)) if self.channels_first else Input(shape=(IMG_W, IMG_H, 3))\n",
    "        x = Conv2D(64, kernel_size=3, padding=\"same\", data_format=self.data_format, kernel_regularizer=l2(self.weight_decay))(input)\n",
    "        for i in range(self.n):\n",
    "            x = self.block(64, x)\n",
    "            \n",
    "        x = self.subsumpling(128, x)\n",
    "        for i in range(self.n):\n",
    "            x = self.block(128, x)\n",
    "        x = self.subsumpling(128, x)\n",
    "        for i in range(self.n):\n",
    "            x = self.block(128, x)\n",
    "            \n",
    "        x = self.subsumpling(256, x)\n",
    "        for i in range(self.n):\n",
    "            x = self.block(256, x)\n",
    "        x = self.subsumpling(256, x)\n",
    "        for i in range(self.n):\n",
    "            x = self.block(256, x)\n",
    "            \n",
    "        x = GlobalAveragePooling2D(data_format=self.data_format)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(256, activation=\"relu\")(x)\n",
    "        x = Dense(N_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "        model = Model(input, x)\n",
    "        return model\n",
    "    \n",
    "    def lr_schduler(self, epoch):\n",
    "        x = self.initial_lr\n",
    "        if epoch >= self.nb_epochs * 0.4: x /= 10.0\n",
    "        if epoch >= self.nb_epochs * 0.6: x /= 10.0\n",
    "        if epoch >= self.nb_epochs * 0.8: x /= 10.0\n",
    "        return x\n",
    "\n",
    "    def train(self, train_generator, validation_generator):\n",
    "        self.model.compile(optimizer=SGD(lr=self.initial_lr, momentum=0.9), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "        train_datagen = ImageDataGenerator(\n",
    "                            rescale=1./255,\n",
    "                            validation_split=0.2)\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "                            PATH,\n",
    "                            target_size=(IMG_W, IMG_H),\n",
    "                            class_mode='categorical',\n",
    "                            subset='training')\n",
    "\n",
    "        validation_generator = train_datagen.flow_from_directory(\n",
    "                            PATH,\n",
    "                            target_size=(IMG_W, IMG_H),\n",
    "                            class_mode='categorical',\n",
    "                            subset='validation')\n",
    "\n",
    "        # Callback\n",
    "        lr_cb = LearningRateScheduler(self.lr_schduler)\n",
    "        tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()),\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_grads=False,\n",
    "                          write_images=False,\n",
    "                          embeddings_freq=0,\n",
    "                          embeddings_layer_names=None,\n",
    "                          embeddings_metadata=None,\n",
    "                          embeddings_data=None,\n",
    "                          update_freq='epoch')\n",
    "\n",
    "        self.history = self.model.fit_generator(train_generator,\n",
    "                                           epochs=self.nb_epochs,\n",
    "                                           steps_per_epoch = 100,\n",
    "                                           callbacks=[lr_cb, tensorboard],\n",
    "                                           validation_data = validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet(n = 3, nb_epochs=50)\n",
    "net.train(train_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "nmNsYXcJ5Z2O",
    "outputId": "ed183351-27fd-4e84-d280-909652423d72"
   },
   "source": [
    "### MISC CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('History', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive('file', 'zip', 'logs')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SPMohanty-Crop-Disease-VGG with BatchNorm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
